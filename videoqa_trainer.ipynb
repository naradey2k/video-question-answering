{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSIib9XGY6Fs",
        "outputId": "1340230f-a0ae-41ca-de5b-a241fa6a9e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-yw1wzvo6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-yw1wzvo6\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (0.15.1+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369398 sha256=52a3987b2d42b8071512b14e155ee0df5907a2bcac08c77c46251a8b5716b0f8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fm6dzlqu/wheels/c8/e4/e1/11374c111387672fc2068dfbe0d4b424cb9cdd1b2e184a71b5\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "joONaR5NqKD1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DjnznvhpILFX"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import os\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as nnf\n",
        "import sys\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from tqdm import tqdm, trange\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import io\n",
        "import os\n",
        "import PIL\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import torchvision\n",
        "import transformers\n",
        "# import more_itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from tqdm.contrib import tzip\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "import io\n",
        "import random\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import sys\n",
        "from tqdm.contrib import tzip\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as nnf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import argparse\n",
        "import json\n",
        "from typing import Tuple, Optional, Union\n",
        "from torch.cuda.amp import autocast\n",
        "from transformers.optimization import Adafactor, AdafactorSchedule\n",
        "import torch\n",
        "import io\n",
        "import os\n",
        "import PIL\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "# import more_itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from dataclasses import dataclass, field\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torch.utils.checkpoint import checkpoint_sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr8n-w8v5noF"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "\n",
        "path = 'train.tar.gz'\n",
        "tar = tarfile.open(path, \"r:gz\")\n",
        "tar.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRYjCDqGAATr"
      },
      "outputs": [],
      "source": [
        "path = '/videos/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKPbg9MbB0ep"
      },
      "outputs": [],
      "source": [
        "def get_vid(idx):\n",
        "    return path + idx + '.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SwUEu4g86xb0",
        "outputId": "bb30aa3a-f40b-4461-daea-d30d0244bbaa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_name</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>okC18bPTils</td>\n",
              "      <td>все люди в видео мужчины</td>\n",
              "      <td>нет</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAQp3iEJxJc</td>\n",
              "      <td>все люди в видео женщины</td>\n",
              "      <td>нет</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>whcxrkF2hws</td>\n",
              "      <td>спортсмен №3, играющий в помещении</td>\n",
              "      <td>нет</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LrputIUn4oY</td>\n",
              "      <td>спортсмен в брюках</td>\n",
              "      <td>нет</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mzt-E6pxuUI</td>\n",
              "      <td>ребенок в помещении</td>\n",
              "      <td>да</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    video_name                            question answer\n",
              "0  okC18bPTils            все люди в видео мужчины    нет\n",
              "1  AAQp3iEJxJc            все люди в видео женщины    нет\n",
              "2  whcxrkF2hws  спортсмен №3, играющий в помещении    нет\n",
              "3  LrputIUn4oY                  спортсмен в брюках    нет\n",
              "4  Mzt-E6pxuUI                 ребенок в помещении     да"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAGDCFZ6aOCA"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wpp4_zdFPzJ"
      },
      "outputs": [],
      "source": [
        "def read_video(path, transform=None, frames_num=16, window=30):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    \n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    \n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    N = length//(frames_num)\n",
        "\n",
        "    print(N, length, frames_num)\n",
        "\n",
        "    current_frame = 1\n",
        "    for i in range(length):\n",
        "\n",
        "        ret, frame = cap.read(current_frame)\n",
        "        if ret and i==current_frame and len(frames)<frames_num:\n",
        "            size = 64, 64\n",
        "            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            frame.thumbnail(size, Image.ANTIALIAS)\n",
        "            frames.append(frame)\n",
        "            current_frame += N\n",
        "      \n",
        "    cap.release()\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S21C4pkyarFN"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRjeRPvia6uv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "data = []\n",
        "for video_name, question, answer in zip(df_train.video_name, df_train.question, df_train.answer):\n",
        "    name = f'videos/{video_name}.mp4'\n",
        "    if os.path.exists(name):  \n",
        "        data += [(name,f'Q: {question} A: {answer}')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBfd7t47psqj",
        "outputId": "5a5f851a-e494-46c0-d606-f541de799d38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('videos/okC18bPTils.mp4', 'Q: все люди в видео мужчины A: нет')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjj0uB4gccCS"
      },
      "outputs": [],
      "source": [
        "def image_grid(imgs, rows, cols):\n",
        "    pils = imgs\n",
        "    \n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "        \n",
        "    return grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uR5W2TTccdv"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "clip_model_type = \"ViT-L/14@336px\"\n",
        "\n",
        "out_path = f\"Features_train_full_ru.pkl\"\n",
        "video_path =  'videos'\n",
        "\n",
        "clip_model, preprocess = clip.load(clip_model_type, device=device, jit=False)\n",
        "clip_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwOK5wascju4"
      },
      "outputs": [],
      "source": [
        "all_embeddings = []\n",
        "all_captions = []\n",
        "i = 0\n",
        "\n",
        "for video_name, question, answer in tzip(df_train.video_name, df_train.question, df_train.answer):\n",
        "    \n",
        "    \n",
        "    name = f'{video_path}/{video_name}.mp4'\n",
        "    \n",
        "    text = f'Q: {question} A: {answer}'\n",
        "    #print(name)\n",
        "    if os.path.exists(name):\n",
        "        \n",
        "        video = read_video(path = name, frames_num=9)\n",
        "        if len(video)>1:\n",
        "            #print(len(video))\n",
        "            image = image_grid(video, 3, 3)\n",
        "\n",
        "            image = preprocess(image).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                prefix = clip_model.encode_image(image).cpu()\n",
        "            #d[\"clip_embedding\"] = i\n",
        "            all_embeddings.append(prefix)\n",
        "            all_captions.append(text)\n",
        "    \n",
        "with open(out_path, 'wb') as f:\n",
        "    pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n",
        "\n",
        "print('Done')\n",
        "print(\"%0d embeddings saved \" % len(all_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uF07hEqcltu"
      },
      "outputs": [],
      "source": [
        "class ClipCocoDataset(Dataset):    \n",
        "    def __init__(self, data_path: str,  prefix_length= 50, gpt2_type = \"sberbank-ai/rugpt3large_based_on_gpt2\",\n",
        "                 normalize_prefix=False):\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
        "        self.prefix_length = prefix_length\n",
        "        self.normalize_prefix = normalize_prefix\n",
        "\n",
        "        with open(data_path, 'rb') as f:\n",
        "            all_data = pickle.load(f)\n",
        "\n",
        "        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n",
        "\n",
        "        sys.stdout.flush()\n",
        "        self.prefixes = all_data[\"clip_embedding\"]\n",
        "        captions_raw = all_data[\"captions\"]\n",
        "        \n",
        "        self.captions = captions_raw   \n",
        "        self.captions_tokens = []\n",
        "        self.caption2embedding = []\n",
        "        max_seq_len = 0        \n",
        "        i = 0\n",
        "\n",
        "        for caption in tqdm(captions_raw):\n",
        "            self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64))\n",
        "            self.caption2embedding.append(self.prefixes[i])\n",
        "            i += 1\n",
        "            max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])          \n",
        "    \n",
        "        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n",
        "\n",
        "        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n",
        "\n",
        "    def pad_tokens(self, item: int):\n",
        "        tokens = self.captions_tokens[item]\n",
        "        padding = self.max_seq_len - tokens.shape[0]\n",
        "\n",
        "        if padding > 0:\n",
        "            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n",
        "            self.captions_tokens[item] = tokens\n",
        "        elif padding < 0:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "            self.captions_tokens[item] = tokens\n",
        "\n",
        "        mask = tokens.ge(0)  \n",
        "        tokens[~mask] = 0\n",
        "        mask = mask.float()\n",
        "        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0) \n",
        "\n",
        "        return tokens, mask\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.captions_tokens)   \n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        tokens, mask = self.pad_tokens(item)\n",
        "        prefix = self.prefixes[item]\n",
        "\n",
        "        if self.normalize_prefix:\n",
        "            prefix = prefix.float()\n",
        "            prefix = prefix / prefix.norm(2, -1)\n",
        "\n",
        "        return tokens, mask, prefix    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6d-G11Bc-Lb",
        "outputId": "fedfc83a-73de-4751-d716-ce473643e7da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data size is 25228\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 25228/25228 [00:04<00:00, 5314.85it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = ClipCocoDataset('Features_train_full_ru.pkl', prefix_length=30, normalize_prefix=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3N3MQ7afj8m"
      },
      "outputs": [],
      "source": [
        "# class MappingType(Enum):\n",
        "#     MLP = 'mlp'\n",
        "#     Transformer = 'transformer'\n",
        "\n",
        "\n",
        "# class MlpTransformer(nn.Module):\n",
        "#      def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):\n",
        "#          super().__init__()\n",
        "#          out_d = out_d if out_d is not None else in_dim\n",
        "#          self.fc1 = nn.Linear(in_dim, h_dim)\n",
        "#          self.act = act\n",
        "#          self.fc2 = nn.Linear(h_dim, out_d)\n",
        "#          self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#      def forward(self, x):\n",
        "#          x = self.fc1(x)\n",
        "#          x = self.act(x)\n",
        "#          x = self.dropout(x)\n",
        "#          x = self.fc2(x)\n",
        "#          x = self.dropout(x)\n",
        "#          return x\n",
        "\n",
        "# class MLP(nn.Module):\n",
        "\n",
        "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "#         return self.model(x)\n",
        "\n",
        "#     def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "#         super(MLP, self).__init__()\n",
        "#         layers = []\n",
        "#         for i in range(len(sizes) - 1):\n",
        "#             layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "#             if i < len(sizes) - 2:\n",
        "#                 layers.append(act())\n",
        "#         self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "\n",
        "#     def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n",
        "#         super().__init__()\n",
        "#         self.num_heads = num_heads\n",
        "#         head_dim = dim_self // num_heads\n",
        "#         self.scale = head_dim ** -0.5\n",
        "#         self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n",
        "#         self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n",
        "#         self.project = nn.Linear(dim_self, dim_self)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, x, y=None, mask=None):\n",
        "#         y = y if y is not None else x\n",
        "#         b, n, c = x.shape\n",
        "#         _, m, d = y.shape\n",
        "#         # b n h dh\n",
        "#         queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n",
        "#         # b m 2 h dh\n",
        "#         keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n",
        "#         keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n",
        "#         attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n",
        "#         if mask is not None:\n",
        "#             if mask.dim() == 2:\n",
        "#                 mask = mask.unsqueeze(1)\n",
        "#             attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n",
        "#         attention = attention.softmax(dim=2)\n",
        "#         out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n",
        "#         out = self.project(out)\n",
        "#         return out, attention\n",
        "\n",
        "\n",
        "# class TransformerLayer(nn.Module):\n",
        "\n",
        "#     def forward_with_attention(self, x, y=None, mask=None):\n",
        "#         x_, attention = self.attn(self.norm1(x), y, mask)\n",
        "#         x = x + x_\n",
        "#         x = x + self.mlp(self.norm2(x))\n",
        "#         return x, attention\n",
        "\n",
        "#     def forward(self, x, y=None, mask=None):\n",
        "#         x = x + self.attn(self.norm1(x), y, mask)[0]\n",
        "#         x = x + self.mlp(self.norm2(x))\n",
        "#         return x\n",
        "\n",
        "#     def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n",
        "#                  norm_layer: nn.Module = nn.LayerNorm):\n",
        "#         super().__init__()\n",
        "#         self.norm1 = norm_layer(dim_self)\n",
        "#         self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n",
        "#         self.norm2 = norm_layer(dim_self)\n",
        "#         self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n",
        "\n",
        "\n",
        "# class Transformer(nn.Module):\n",
        "\n",
        "#     def forward_with_attention(self, x, y=None, mask=None):\n",
        "#         attentions = []\n",
        "#         for layer in self.layers:\n",
        "#             x, att = layer.forward_with_attention(x, y, mask)\n",
        "#             attentions.append(att)\n",
        "#         return x, attentions\n",
        "\n",
        "#     def forward(self, x, y=None, mask=None):\n",
        "#         for i, layer in enumerate(self.layers):\n",
        "#             if i % 2 == 0 and self.enc_dec: # cross\n",
        "#                 x = layer(x, y)\n",
        "#             elif self.enc_dec:  # self\n",
        "#                 x = layer(x, x, mask)\n",
        "#             else:  # self or cross\n",
        "#                 x = layer(x, y, mask)\n",
        "#         return x\n",
        "\n",
        "#     def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n",
        "#                  mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n",
        "#         super(Transformer, self).__init__()\n",
        "#         dim_ref = dim_ref if dim_ref is not None else dim_self\n",
        "#         self.enc_dec = enc_dec\n",
        "#         if enc_dec:\n",
        "#             num_layers = num_layers * 2\n",
        "#         layers = []\n",
        "#         for i in range(num_layers):\n",
        "#             if i % 2 == 0 and enc_dec:  # cross\n",
        "#                 layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "#             elif enc_dec:  # self\n",
        "#                 layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "#             else:  # self or cross\n",
        "#                 layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "#         self.layers = nn.ModuleList(layers)\n",
        "\n",
        "\n",
        "# class TransformerMapper(nn.Module):\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n",
        "#         prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n",
        "#         prefix = torch.cat((x, prefix), dim=1)\n",
        "#         out = self.transformer(prefix)[:, self.clip_length:]\n",
        "#         return out\n",
        "\n",
        "#     def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n",
        "#         super(TransformerMapper, self).__init__()\n",
        "#         self.clip_length = clip_length\n",
        "#         self.transformer = Transformer(dim_embedding, 8, num_layers)\n",
        "#         self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n",
        "#         self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)\n",
        "\n",
        "\n",
        "# class ClipCaptionModel(nn.Module):\n",
        "\n",
        "#     def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
        "#         return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "#     def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
        "#                 labels: Optional[torch.Tensor] = None):\n",
        "#         embedding_text = self.gpt.transformer.wte(tokens)\n",
        "#         prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "#         embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "#         if labels is not None:\n",
        "#             dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "#             labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "#         out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "#         return out\n",
        "\n",
        "#     def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n",
        "#                  num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n",
        "#         super(ClipCaptionModel, self).__init__()\n",
        "#         self.prefix_length = prefix_length\n",
        "#         self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "#         self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "#         if mapping_type == MappingType.MLP:\n",
        "#             self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
        "#                                      self.gpt_embedding_size * prefix_length))\n",
        "#         else:\n",
        "#             self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n",
        "#                                                                      clip_length, num_layers)\n",
        "\n",
        "\n",
        "# class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "#     def parameters(self, recurse: bool = True):\n",
        "#         return self.clip_project.parameters()\n",
        "\n",
        "#     def train(self, mode: bool = True):\n",
        "#         super(ClipCaptionPrefix, self).train(mode)\n",
        "#         self.gpt.eval()\n",
        "#         return self\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) - 1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "    \n",
        "def freeze(\n",
        "    model,\n",
        "    freeze_emb=False,\n",
        "    freeze_ln=False,\n",
        "    freeze_attn=True,\n",
        "    freeze_ff=True,\n",
        "    freeze_other=True,\n",
        "):\n",
        "    \n",
        "    for name, p in model.named_parameters():\n",
        "    # freeze all parameters except the layernorm and positional embeddings\n",
        "       \n",
        "       \n",
        "        \n",
        "        name = name.lower()\n",
        "        if 'ln' in name or 'norm' in name:\n",
        "            p.requires_grad = not freeze_ln\n",
        "        elif 'embeddings' in name:\n",
        "            p.requires_grad = not freeze_emb\n",
        "        elif 'mlp' in name:\n",
        "            p.requires_grad = not freeze_ff\n",
        "        elif 'attn' in name:\n",
        "            p.requires_grad = not freeze_attn\n",
        "        else:\n",
        "            p.requires_grad = not freeze_other\n",
        "           \n",
        "    return model\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "    def __init__(self, backbone, prefix_length: int, prefix_size: int = 768):\n",
        "          super(ClipCaptionModel, self).__init__()\n",
        "          self.prefix_length = prefix_length\n",
        "\n",
        "          self.gpt = GPT2LMHeadModel.from_pretrained(backbone)\n",
        "          #self.gpt = freeze(self.gpt)\n",
        "          self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "          self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
        "                                  self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "    \n",
        "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
        "                labels: Optional[torch.Tensor] = None):\n",
        "\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "\n",
        "        return self\n",
        "\n",
        "def train(dataset, model: ClipCaptionModel, args,\n",
        "          warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
        "  \n",
        "    device = torch.device('cuda')\n",
        "    batch_size = args.bs\n",
        "    epochs = args.epochs\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    \n",
        "    model = model.to(device)\n",
        "    \n",
        "    model.train()\n",
        "    optimizer = AdamW(model.parameters(), lr=args.lr,betas=(0.9, 0.995))\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\">>> Training epoch {epoch}\")\n",
        "        sys.stdout.flush()\n",
        "        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
        "        step = 0\n",
        "\n",
        "        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n",
        "            model.zero_grad()\n",
        "            step+=1\n",
        "            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
        "            \n",
        "            outputs = model(tokens, prefix, mask)\n",
        "            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
        "\n",
        "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n",
        "            segments = 2\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            progress.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "                        \n",
        "            progress.update()            \n",
        "\n",
        "            del tokens\n",
        "            del mask\n",
        "            del prefix\n",
        "            torch.clear_autocast_cache()\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "            if (idx + 1) % 7000 == 0:\n",
        "                torch.save(\n",
        "                    model.state_dict(),\n",
        "                    \n",
        "                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
        "                )\n",
        "\n",
        "        progress.close()\n",
        "        if epoch % args.save_every ==0:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n",
        "            )\n",
        "\n",
        "    return model\n",
        "\n",
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.backbone = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
        "        self.data = 'Features_train_full_ru.pkl'\n",
        "        self.out_dir = 'checkpoints_larger'\n",
        "        self.prefix = 'prefix_1'\n",
        "        self.epochs = 10\n",
        "        self.save_every = 1\n",
        "        self.prefix_length = 30\n",
        "        self.bs = 10\n",
        "        self.only_prefix = False\n",
        "        self.lr = 2e-5       \n",
        "\n",
        "def main():    \n",
        "    args = Args()\n",
        "\n",
        "    prefix_length = args.prefix_length\n",
        "    dataset = ClipCocoDataset(args.data, prefix_length)\n",
        "    \n",
        "    model = ClipCaptionModel(backbone = args.backbone, prefix_length = prefix_length)\n",
        "    \n",
        "    print(\"Train both prefix and GPT\")\n",
        "    sys.stdout.flush()\n",
        "    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT6UdHwPhJGa",
        "scrolled": true,
        "outputId": "352f1a5e-20a7-4e67-8235-fd44f4732568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data size is 25228\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/25228 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  2%|▏         | 537/25228 [00:00<00:04, 5367.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|▍         | 1074/25228 [00:00<00:04, 5318.52it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  7%|▋         | 1730/25228 [00:00<00:03, 5880.63it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  9%|▉         | 2319/25228 [00:00<00:04, 5680.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 3008/25228 [00:00<00:03, 6103.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|█▍        | 3621/25228 [00:00<00:04, 5087.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 17%|█▋        | 4165/25228 [00:00<00:04, 5187.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 19%|█▉        | 4737/25228 [00:00<00:03, 5340.29it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 21%|██        | 5294/25228 [00:00<00:03, 5405.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▎       | 5979/25228 [00:01<00:03, 5829.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 26%|██▌       | 6571/25228 [00:01<00:03, 4840.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 7133/25228 [00:01<00:03, 5041.25it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 30%|███       | 7664/25228 [00:01<00:03, 4781.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 33%|███▎      | 8257/25228 [00:01<00:03, 5084.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 35%|███▌      | 8890/25228 [00:01<00:03, 5425.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 37%|███▋      | 9452/25228 [00:01<00:02, 5479.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 10129/25228 [00:01<00:02, 5849.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 43%|████▎     | 10797/25228 [00:01<00:02, 6089.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 45%|████▌     | 11464/25228 [00:02<00:02, 6258.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 12147/25228 [00:02<00:02, 6425.54it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 51%|█████     | 12794/25228 [00:02<00:02, 5721.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 53%|█████▎    | 13384/25228 [00:02<00:02, 5202.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 55%|█████▌    | 13923/25228 [00:02<00:02, 4792.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 57%|█████▋    | 14419/25228 [00:02<00:02, 4350.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 59%|█████▉    | 14870/25228 [00:02<00:02, 3784.19it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 61%|██████    | 15268/25228 [00:03<00:02, 3549.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▏   | 15700/25228 [00:03<00:02, 3730.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 16184/25228 [00:03<00:02, 4008.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|██████▌   | 16681/25228 [00:03<00:02, 4261.36it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 17121/25228 [00:03<00:01, 4169.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 70%|██████▉   | 17570/25228 [00:03<00:01, 4256.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 18085/25228 [00:03<00:01, 4508.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 74%|███████▎  | 18543/25228 [00:03<00:01, 4226.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 75%|███████▌  | 18974/25228 [00:03<00:01, 3986.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 77%|███████▋  | 19380/25228 [00:04<00:01, 3914.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|███████▊  | 19776/25228 [00:04<00:01, 3862.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|███████▉  | 20173/25228 [00:04<00:01, 3890.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 82%|████████▏ | 20600/25228 [00:04<00:01, 3996.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▎ | 21092/25228 [00:04<00:00, 4261.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|████████▌ | 21654/25228 [00:04<00:00, 4657.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 22186/25228 [00:04<00:00, 4851.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|████████▉ | 22683/25228 [00:04<00:00, 4886.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 23227/25228 [00:04<00:00, 5050.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 94%|█████████▍| 23815/25228 [00:04<00:00, 5291.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 97%|█████████▋| 24349/25228 [00:05<00:00, 5305.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 25228/25228 [00:05<00:00, 4875.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "prefix_1:  13%|█▎        | 316/2522 [20:54<2:26:00,  3.97s/it, loss=10.7]\n",
            "prefix_1:  54%|█████▍    | 1356/2522 [23:15<19:59,  1.03s/it, loss=5.82]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train both prefix and GPT\n",
            ">>> Training epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "prefix_1: 100%|██████████| 2522/2522 [11:30<00:00,  3.65it/s, loss=1.17] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Training epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "prefix_1: 100%|██████████| 2522/2522 [11:33<00:00,  3.64it/s, loss=1.2]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Training epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "prefix_1: 100%|██████████| 2522/2522 [11:31<00:00,  3.65it/s, loss=1]    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Training epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "prefix_1: 100%|██████████| 2522/2522 [11:28<00:00,  3.66it/s, loss=0.994]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Training epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "prefix_1: 100%|██████████| 2522/2522 [11:33<00:00,  3.64it/s, loss=0.79] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Training epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "prefix_1: 100%|██████████| 2522/2522 [11:30<00:00,  3.65it/s, loss=0.969]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Training epoch 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "prefix_1: 100%|██████████| 2522/2522 [11:29<00:00,  3.66it/s, loss=0.72] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Training epoch 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "prefix_1: 100%|██████████| 2522/2522 [11:29<00:00,  3.66it/s, loss=0.394]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Training epoch 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "prefix_1: 100%|██████████| 2522/2522 [11:30<00:00,  3.65it/s, loss=0.463]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Training epoch 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "prefix_1: 100%|██████████| 2522/2522 [11:31<00:00,  3.65it/s, loss=0.657]\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyO_SyvJAepw"
      },
      "outputs": [],
      "source": [
        "!cp /content/checkpoints/prefix_1-007.pt /content/drive/MyDrive/4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_hej1zYCopu"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/4/prefix_1-007.pt ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqNobiKVGTVi"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "clip_model, preprocess = clip.load(\"ViT-L/14@336px\", device=device, jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('sberbank-ai/rugpt3large_based_on_gpt2')\n",
        "\n",
        "prefix_length= 50\n",
        "model_path = 'checkpoints_larger/prefix_1-004.pt'\n",
        "model = ClipCaptionModel(backbone = 'sberbank-ai/rugpt3small_based_on_gpt2', prefix_length = 50)\n",
        "\n",
        "model.load_state_dict(torch.load(model_path, map_location='cpu')) \n",
        "model.to(device)\n",
        "None"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}