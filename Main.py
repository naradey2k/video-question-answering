import torch
import numpy as np
import pandas as pd
import streamlit as st

import matplotlib.pyplot as plt
import seaborn as sns

import torch
import torch.nn as nn
from torch.nn import functional as nnf
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from tqdm import tqdm, trange
import os
import pickle
import sys
import argparse
import json
from typing import Tuple, Optional, Union
# from torch.cuda.amp import autocast
import io
import os
import PIL
import cv2
import tempfile
import streamlit as st

from model_inference import *
from model import ClipCaptionModel

device = 'cpu'
clip_model, preprocess = clip.load("ViT-L/14@336px", device=device, jit=False)
tokenizer = GPT2Tokenizer.from_pretrained('sberbank-ai/rugpt3large_based_on_gpt2')

prefix_length= 50
model_path = 'transformerfr-004.pt'
model = ClipCaptionModel('sberbank-ai/rugpt3small_based_on_gpt2', prefix_length = 50)

model.load_state_dict(torch.load(model_path, map_location='cpu'))
model.to(device)

st.set_page_config(
    page_title="Video Analysis AI",
    page_icon="üéà",
)

def _max_width_():
    max_width_str = f"max-width: 1400px;"
    st.markdown(
        f"""
    <style>
    .reportview-container .main .block-container{{
        {max_width_str}
    }}
    </style>
    """,
        unsafe_allow_html=True,
    )

_max_width_()


def main():
    st.title("ü§ñ VideoQA")
    st.header("")

    with st.sidebar.expander("‚ÑπÔ∏è - –û –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏", expanded=True):
        st.write(
            """
    -   *VideoQA* —Å—Ç—Ä–µ–º–∏—Ç—Å—è –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –≤–∏–¥–µ–æ!
    -   –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –Ω–∞—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω–æ–º –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –¥–∞–Ω–Ω—ã—Ö ActivityNet –∏ –æ–±—É—á–µ–Ω–∞ –ø—Ä–∏ –ø–æ–º–æ—â–∏ –∞—Ä—Ö–∏—Ç–µ–∫—É—Ä ruGPT3-large –∏ OpenAI CLIP
    	    """
        )


    uploaded_file = st.file_uploader("üìå –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∏–¥–µ–æ", ['.mp4'])

    play_video = st.checkbox("–û—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ")

    if play_video:
        video_bytes = uploaded_file.read()
        st.video(video_bytes)

    st.write("---")

    question = st.text_input("üìå –í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –¥–ª—è –≤–∏–¥–µ–æ", "")


    tfile = tempfile.NamedTemporaryFile(delete=False)
    tfile.write(uploaded_file.read())

    device = 'cpu'
    val_embeddings = []
    val_captions = []
    result = ''
    text = f'Question: {question}? Answer:'
    video = read_video(tfile.name, transform=None, frames_num=4)

    if len(video) > 0:
        i = image_grid(video, 2, 2)
        image = preprocess(i).unsqueeze(0).to(device)

        with torch.no_grad():
            prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)

        val_embeddings.append(prefix)
        val_captions.append(text)

    answers = []

    for i in tqdm(range(len(val_embeddings))):
        emb = val_embeddings[i]
        caption = val_captions[i]

        ans = get_ans(model, tokenizer, emb, prefix_length, caption)
        answers.append(ans['answer'])

    result = answers[0].split(' A: ')[0]

    res = st.text_input('üìå –û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å', result, disabled=False)


if __name__ == '__main__':
    main()
